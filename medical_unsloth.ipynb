{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23d845c",
   "metadata": {},
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
    "\n",
    "[NEW] Supports all Qwen 2.5 model sizes! 0.5, 1.5, 3, 7, 14, 32, 72b!\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb357f",
   "metadata": {},
   "source": [
    "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
    "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
    "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
    "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
    "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
    "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d262404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.4: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8158e322fbc4d689166860ec6a70cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f959420759148f49ecc7270ccb5a8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5244e96a40af4e8e9d42be64e7da40aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a84e3dfe8d40dea58e3a4cbb6ecf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02841b9670f4c5ca4287552be49580e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6166e7747c4749de92c278355e2b14d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca0b0b5dd7440029a6e677c82d8779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0157a119f44e0caaaeb63426fc32c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # Can select any from the below:\n",
    "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "    # And also all Instruct versions and Math. Coding verisons!\n",
    "    model_name = \"unsloth/Qwen2.5-1.5B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ae43e",
   "metadata": {},
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3baca",
   "metadata": {},
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
    "\n",
    "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "{'system': \"You are a helpful medical assistant.\\nAnswer medical questions clearly and concisely in English.\\nIf the question is dangerous (like medication dosages for children), reply with: 'Consult a qualified doctor.'\", 'instruction': 'Question: What are the treatments for progressive external ophthalmoplegia ?', 'input': '', 'output': 'These resources address the diagnosis or management of progressive external ophthalmoplegia:  - Gene Review: Gene Review: Mitochondrial DNA Deletion Syndromes  - Gene Review: Gene Review: POLG-Related Disorders  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 1  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 2  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 3  - Genetic Testing Registry: Progressive external ophthalmoplegia  - United Mitochondrial Disease Foundation: Diagnosis of Mitochondrial Disease   These resources from MedlinePlus offer information about the diagnosis and management of various health conditions:  - Diagnostic Tests  - Drug Therapy  - Surgery and Rehabilitation  - Genetic Counseling   - Palliative Care', 'history': []}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"/content/train.json\" #dir of data file\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "print( len(data))\n",
    "print( data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51adfc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968258eebbd6493088abfbfe25173ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "# dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(data)\n",
    "dataset = train_dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b209c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': \"You are a helpful medical assistant.\\nAnswer medical questions clearly and concisely in English.\\nIf the question is dangerous (like medication dosages for children), reply with: 'Consult a qualified doctor.'\",\n",
       " 'instruction': 'Question: What are the treatments for progressive external ophthalmoplegia ?',\n",
       " 'input': '',\n",
       " 'output': 'These resources address the diagnosis or management of progressive external ophthalmoplegia:  - Gene Review: Gene Review: Mitochondrial DNA Deletion Syndromes  - Gene Review: Gene Review: POLG-Related Disorders  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 1  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 2  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 3  - Genetic Testing Registry: Progressive external ophthalmoplegia  - United Mitochondrial Disease Foundation: Diagnosis of Mitochondrial Disease   These resources from MedlinePlus offer information about the diagnosis and management of various health conditions:  - Diagnostic Tests  - Drug Therapy  - Surgery and Rehabilitation  - Genetic Counseling   - Palliative Care',\n",
       " 'history': [],\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nQuestion: What are the treatments for progressive external ophthalmoplegia ?\\n\\n### Input:\\n\\n\\n### Response:\\nThese resources address the diagnosis or management of progressive external ophthalmoplegia:  - Gene Review: Gene Review: Mitochondrial DNA Deletion Syndromes  - Gene Review: Gene Review: POLG-Related Disorders  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 1  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 2  - Genetic Testing Registry: Autosomal dominant progressive external ophthalmoplegia with mitochondrial DNA deletions 3  - Genetic Testing Registry: Progressive external ophthalmoplegia  - United Mitochondrial Disease Foundation: Diagnosis of Mitochondrial Disease   These resources from MedlinePlus offer information about the diagnosis and management of various health conditions:  - Diagnostic Tests  - Drug Therapy  - Surgery and Rehabilitation  - Genetic Counseling   - Palliative Care<|im_end|>'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e2e4e",
   "metadata": {},
   "source": [
    "train.json have been used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d80d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 5751\n",
      "Mean length: 325\n",
      "95% percentile: 808\n",
      "99% percentile: 1707\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for sample in dataset:\n",
    "    tokens = tokenizer(\n",
    "        sample[\"text\"],\n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )[\"input_ids\"]\n",
    "    lengths.append(len(tokens))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lengths = np.array(lengths)\n",
    "\n",
    "print(\"Max length:\", lengths.max())\n",
    "print(\"Mean length:\", int(lengths.mean()))\n",
    "print(\"95% percentile:\", int(np.percentile(lengths, 95)))\n",
    "print(\"99% percentile:\", int(np.percentile(lengths, 99)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8a2d7",
   "metadata": {},
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89f256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b68e1b1a7494947a42c2b849d8023ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    # formatting_func=formatting_func,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        num_train_epochs = 3.0,\n",
    "        learning_rate = 1e-4,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        warmup_ratio = 0.1,\n",
    "        save_steps = 500,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "5.764 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbdf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 16,000 | Num Epochs = 3 | Total steps = 6,000\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 73,859,072 of 1,617,573,376 (4.57% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 3:10:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.881200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.920600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.826500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.878600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.788700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.768300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.782900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.759900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.769700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.750500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.657600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.647100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996d798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/outputs/ (stored 0%)\n",
      "  adding: content/outputs/README.md (deflated 45%)\n",
      "  adding: content/outputs/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: content/outputs/checkpoint-6000/ (stored 0%)\n",
      "  adding: content/outputs/checkpoint-6000/README.md (deflated 65%)\n",
      "  adding: content/outputs/checkpoint-6000/adapter_model.safetensors (deflated 7%)\n",
      "  adding: content/outputs/checkpoint-6000/tokenizer_config.json (deflated 89%)\n",
      "  adding: content/outputs/checkpoint-6000/vocab.json (deflated 61%)\n",
      "  adding: content/outputs/checkpoint-6000/special_tokens_map.json (deflated 69%)\n",
      "  adding: content/outputs/checkpoint-6000/adapter_config.json (deflated 57%)\n",
      "  adding: content/outputs/checkpoint-6000/chat_template.jinja (deflated 71%)\n",
      "  adding: content/outputs/checkpoint-6000/training_args.bin (deflated 53%)\n",
      "  adding: content/outputs/checkpoint-6000/tokenizer.json (deflated 81%)\n",
      "  adding: content/outputs/checkpoint-6000/rng_state.pth (deflated 26%)\n",
      "  adding: content/outputs/checkpoint-6000/optimizer.pt (deflated 12%)\n",
      "  adding: content/outputs/checkpoint-6000/merges.txt (deflated 57%)\n",
      "  adding: content/outputs/checkpoint-6000/trainer_state.json (deflated 79%)\n",
      "  adding: content/outputs/checkpoint-6000/added_tokens.json (deflated 67%)\n",
      "  adding: content/outputs/checkpoint-6000/scaler.pt (deflated 64%)\n",
      "  adding: content/outputs/checkpoint-6000/scheduler.pt (deflated 62%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r outputs.zip /content/outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"outputs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6967cf",
   "metadata": {},
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"lora_model\") # Local saving\n",
    "# tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"A7medsala7/Medical_model_using_unsloth\", token = \"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\") # Online saving\n",
    "# tokenizer.push_to_hub(\"A7medsala7/Medical_model_using_unsloth\", token = \"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\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005990d75bdc4c389248464f490d2fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0c916a9d0a4522be7af60a8ca3dc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:52<00:00, 52.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:55<00:00, 55.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/model`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF f16 to ['f16'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: All required system packages already installed!\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Cloning llama.cpp repository\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth: Successfully installed llama.cpp!\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into f16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['qwen2.5-1.5b-instruct.F16.gguf']\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['qwen2.5-1.5b-instruct.F16.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model qwen2.5-1.5b-instruct.F16.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'save_directory': 'model',\n",
       " 'gguf_files': ['qwen2.5-1.5b-instruct.F16.gguf'],\n",
       " 'modelfile_location': '/content/Modelfile',\n",
       " 'want_full_precision': True,\n",
       " 'is_vlm': False,\n",
       " 'fix_bos_token': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b56a640",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a4c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting model to GGUF format...\n",
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e6fc24ba8043c996f8879c939c9583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:40<00:00, 100.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:24<00:00, 84.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/unsloth_gguf_fnxmwcwi`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF f16 to ['f16'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into f16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['qwen2.5-1.5b-instruct.F16.gguf']\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['qwen2.5-1.5b-instruct.F16.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model qwen2.5-1.5b-instruct.F16.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n",
      "Uploading qwen2.5-1.5b-instruct.F16.gguf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c443f8c270ff4a52a9d066d5cfa195e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47dd79b72c904ac1898248dcaa430fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bfcfd0f12c4a75b04187563b50b42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ....5-1.5b-instruct.F16.gguf:   1%|1         | 42.0MB / 3.09GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading config.json...\n",
      "Uploading Ollama Modelfile...\n",
      "Unsloth: Successfully uploaded GGUF to https://huggingface.co/A7medsala7/Medical_model_using_unsloth\n",
      "Unsloth: Cleaning up temporary files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A7medsala7/Medical_model_using_unsloth'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub_gguf(\"A7medsala7/Medical_model_using_unsloth\", tokenizer, quantization_method = \"f16\", token = \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ec0b6",
   "metadata": {},
   "source": [
    "using ollama pull and add the link of the model from hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff9b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.4: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"model\",\n",
    "    load_in_4bit=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the standard treatment for Erdheim‚ÄìChester disease?\n",
      "A: There is no standard treatment for Erdheim‚ÄìChester disease. Treatment is based on the signs and symptoms present in each person.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What is the most common mutation associated with Erdheim‚ÄìChester disease?\n",
      "A: The most common mutation associated with Erdheim‚ÄìChester disease is a mutation in the gene encoding the protein alpha ()-synuclein. This protein is normally found in the brain and other parts of the body, but in people with Erdheim‚ÄìChester disease, it accumulates in abnormal clumps within the body's tissues. The exact mechanism by which this accumulation leads to the signs and symptoms of the disease is not fully understood.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: How does Erdheim‚ÄìChester disease differ from Langerhans cell histiocytosis in terms of cell involvement?\n",
      "A: Erdheim-Chester disease and Langerhans cell histiocytosis are two different diseases that involve different types of cells. In Erdheim-Chester disease, the cells involved are histiocytes, which are a type of immune cell. In Langerhans cell histiocytosis, the cells involved are Langerhans cells, which are also a type of immune cell.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What role does interferon-alpha play in the treatment of Erdheim‚ÄìChester disease?\n",
      "A: Interferon-alpha is used to treat some types of cancer, including non-Hodgkin lymphoma and Kaposi sarcoma. It is also used to treat some types of chronic hepatitis. Interferon-alpha is also used to treat some types of autoimmune diseases, including rheumatoid arthritis and systemic lupus erythematosus. Interferon-alpha is also used to treat some types of skin disorders, including psoriasis and vitiligo.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: List some common cardiovascular or neurological complications of Erdheim‚ÄìChester disease.\n",
      "A: Erdheim‚ÄìChester disease can cause complications in the cardiovascular and neurological systems. The most common cardiovascular complications are arrhythmia, myocarditis, and pericarditis. The most common neurological complications are dementia, seizures, and focal neurological deficits.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: A 45-year-old patient presents with persistent bone pain, and X-rays show bilateral cortical sclerosis of long bones. What differential diagnoses should be considered?\n",
      "A: The differential diagnoses for osteoporosis include: 1. Osteogenesis imperfecta 2. Paget's disease 3. Osteoporosis with multiple myeloma 4. Osteoporosis with sarcoidosis 5. Osteoporosis with rheumatoid arthritis 6. Osteoporosis with hemochromatosis 7. Osteoporosis with sarcoidosis\n",
      "--------------------------------------------------------------------------------\n",
      "Q: A patient with Erdheim‚ÄìChester disease is found to have a BRAF V600E mutation. What targeted therapy is appropriate?\n",
      "A: BRAF inhibitors are approved for the treatment of patients with metastatic melanoma who have a BRAF V600E mutation. However, there is limited information regarding the use of these inhibitors in patients with non-melanoma tumors, including Erdheim‚ÄìChester disease.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: If no BRAF mutation is detected, what alternative targeted therapy could be considered?\n",
      "A: If no BRAF mutation is detected, other targeted therapies may be considered. These include MEK inhibitors such as trametinib and dabrafenib.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: In what situations might radiation therapy be useful for Erdheim‚ÄìChester disease?\n",
      "A: Radiation therapy is not typically used to treat Erdheim‚ÄìChester disease.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: How would you explain Erdheim‚ÄìChester disease in simple terms to a high school student?\n",
      "A: Erdheim-Chester disease is a rare disease that affects the lymph nodes, spleen, and other organs. It is characterized by the presence of abnormal cells called histiocytes, which are normally found in the bone marrow and lymph nodes. These cells can accumulate in the affected organs and cause inflammation and tissue damage. The exact cause of Erdheim-Chester disease is unknown, but it is thought to be related to an autoimmune disorder in which the body's immune system mistakenly attacks its own tissues.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Is there a single globally standardized protocol for Erdheim‚ÄìChester disease approved by the FDA?\n",
      "A: There is no single globally standardized protocol for the diagnosis and treatment of Erdheim-Chester disease (ECD). The management of ECD is based on the clinical symptoms and severity of the disease.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Which targeted therapy for Erdheim‚ÄìChester disease received FDA approval in 2017?\n",
      "A: Daratumumab\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Can conventional chemotherapy (e.g., doxorubicin or cyclophosphamide) be considered a standard treatment for Erdheim‚ÄìChester disease today?\n",
      "A: Conventional chemotherapy is not a standard treatment for Erdheim‚ÄìChester disease (ECD). ECD is a rare, chronic, granulomatous disease of the lymph nodes and other tissues that is characterized by the presence of specific histopathological features. The disease is usually treated with immunomodulatory drugs, such as rituximab, and/or steroids.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Who won the last Liverpool football match?\n",
      "A: As an AI assistant, I can't provide you with real-time information. You can check the latest results of Liverpool football matches on sports news websites or apps.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What is the current exchange rate of USD to Egyptian Pound today?\n",
      "A: As of today, the exchange rate of USD to Egyptian Pound is 1 USD = 18.50 EGP.\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What is the recipe for chocolate cake?\n",
      "A: Sure, here is a simple recipe for chocolate cake. You will need the following ingredients: - 1 cup all-purpose flour  - 1/2 cup unsalted butter  - 2 cups sugar  - 4 large eggs  - 2 cups whole milk  - 2 teaspoons baking soda  - 1 teaspoon salt  - 2 teaspoons vanilla extract  - 2 cups hot water  - 1 cup semisweet chocolate chips  - 1 cup heavy cream  - 1/2 cup unsalted butter  - 1 cup powdered sugar  - 1 teaspoon vanilla extract  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped cashews  - 1 cup chopped almonds  - 1 cup chopped hazelnuts  - 1 cup chopped walnuts  - 1 cup chopped pecans  - 1 cup chopped\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluation_questions = [\n",
    "    \"What is the standard treatment for Erdheim‚ÄìChester disease?\",\n",
    "    \"What is the most common mutation associated with Erdheim‚ÄìChester disease?\",\n",
    "    \"How does Erdheim‚ÄìChester disease differ from Langerhans cell histiocytosis in terms of cell involvement?\",\n",
    "    \"What role does interferon-alpha play in the treatment of Erdheim‚ÄìChester disease?\",\n",
    "    \"List some common cardiovascular or neurological complications of Erdheim‚ÄìChester disease.\",\n",
    "    \"A 45-year-old patient presents with persistent bone pain, and X-rays show bilateral cortical sclerosis of long bones. What differential diagnoses should be considered?\",\n",
    "    \"A patient with Erdheim‚ÄìChester disease is found to have a BRAF V600E mutation. What targeted therapy is appropriate?\",\n",
    "    \"If no BRAF mutation is detected, what alternative targeted therapy could be considered?\",\n",
    "    \"In what situations might radiation therapy be useful for Erdheim‚ÄìChester disease?\",\n",
    "    \"How would you explain Erdheim‚ÄìChester disease in simple terms to a high school student?\",\n",
    "    \"Is there a single globally standardized protocol for Erdheim‚ÄìChester disease approved by the FDA?\",\n",
    "    \"Which targeted therapy for Erdheim‚ÄìChester disease received FDA approval in 2017?\",\n",
    "    \"Can conventional chemotherapy (e.g., doxorubicin or cyclophosphamide) be considered a standard treatment for Erdheim‚ÄìChester disease today?\",\n",
    "    \"Who won the last Liverpool football match?\",\n",
    "    \"What is the current exchange rate of USD to Egyptian Pound today?\",\n",
    "    \"What is the recipe for chocolate cake?\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for q in evaluation_questions:\n",
    "    # Prepare chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "     max_new_tokens=1024,\n",
    "     do_sample=False, top_k=None, temperature=None, top_p=None,\n",
    "    )\n",
    "\n",
    "    # Remove the prompt tokens\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    responses.append({\"question\": q, \"answer\": response})\n",
    "\n",
    "# Show results\n",
    "for r in responses:\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    print(\"A:\", r[\"answer\"])\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdda78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"What is the standard treatment for Erdheim\\u2013Chester disease?\",\n          \"What is the most common mutation associated with Erdheim\\u2013Chester disease?\",\n          \"A 45-year-old patient presents with persistent bone pain, and X-rays show bilateral cortical sclerosis of long bones. What differential diagnoses should be considered?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"There is no standard treatment for Erdheim\\u2013Chester disease. Treatment is based on the signs and symptoms present in each person.\",\n          \"The most common mutation associated with Erdheim\\u2013Chester disease is a mutation in the gene encoding the protein alpha ()-synuclein. This protein is normally found in the brain and other parts of the body, but in people with Erdheim\\u2013Chester disease, it accumulates in abnormal clumps within the body's tissues. The exact mechanism by which this accumulation leads to the signs and symptoms of the disease is not fully understood.\",\n          \"The differential diagnoses for osteoporosis include: 1. Osteogenesis imperfecta 2. Paget's disease 3. Osteoporosis with multiple myeloma 4. Osteoporosis with sarcoidosis 5. Osteoporosis with rheumatoid arthritis 6. Osteoporosis with hemochromatosis 7. Osteoporosis with sarcoidosis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-f0ec8664-a757-4b9e-aebe-aaaa425aaba3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the standard treatment for Erdheim‚ÄìChe...</td>\n",
       "      <td>There is no standard treatment for Erdheim‚ÄìChe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the most common mutation associated wi...</td>\n",
       "      <td>The most common mutation associated with Erdhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does Erdheim‚ÄìChester disease differ from L...</td>\n",
       "      <td>Erdheim-Chester disease and Langerhans cell hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What role does interferon-alpha play in the tr...</td>\n",
       "      <td>Interferon-alpha is used to treat some types o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List some common cardiovascular or neurologica...</td>\n",
       "      <td>Erdheim‚ÄìChester disease can cause complication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A 45-year-old patient presents with persistent...</td>\n",
       "      <td>The differential diagnoses for osteoporosis in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A patient with Erdheim‚ÄìChester disease is foun...</td>\n",
       "      <td>BRAF inhibitors are approved for the treatment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>If no BRAF mutation is detected, what alternat...</td>\n",
       "      <td>If no BRAF mutation is detected, other targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In what situations might radiation therapy be ...</td>\n",
       "      <td>Radiation therapy is not typically used to tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How would you explain Erdheim‚ÄìChester disease ...</td>\n",
       "      <td>Erdheim-Chester disease is a rare disease that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Is there a single globally standardized protoc...</td>\n",
       "      <td>There is no single globally standardized proto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Which targeted therapy for Erdheim‚ÄìChester dis...</td>\n",
       "      <td>Daratumumab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Can conventional chemotherapy (e.g., doxorubic...</td>\n",
       "      <td>Conventional chemotherapy is not a standard tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Who won the last Liverpool football match?</td>\n",
       "      <td>As an AI assistant, I can't provide you with r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the current exchange rate of USD to Eg...</td>\n",
       "      <td>As of today, the exchange rate of USD to Egypt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the recipe for chocolate cake?</td>\n",
       "      <td>Sure, here is a simple recipe for chocolate ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0ec8664-a757-4b9e-aebe-aaaa425aaba3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f0ec8664-a757-4b9e-aebe-aaaa425aaba3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f0ec8664-a757-4b9e-aebe-aaaa425aaba3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_0d1b5591-4234-4960-92e9-e334aeb34c5b\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_0d1b5591-4234-4960-92e9-e334aeb34c5b button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the standard treatment for Erdheim‚ÄìChe...   \n",
       "1   What is the most common mutation associated wi...   \n",
       "2   How does Erdheim‚ÄìChester disease differ from L...   \n",
       "3   What role does interferon-alpha play in the tr...   \n",
       "4   List some common cardiovascular or neurologica...   \n",
       "5   A 45-year-old patient presents with persistent...   \n",
       "6   A patient with Erdheim‚ÄìChester disease is foun...   \n",
       "7   If no BRAF mutation is detected, what alternat...   \n",
       "8   In what situations might radiation therapy be ...   \n",
       "9   How would you explain Erdheim‚ÄìChester disease ...   \n",
       "10  Is there a single globally standardized protoc...   \n",
       "11  Which targeted therapy for Erdheim‚ÄìChester dis...   \n",
       "12  Can conventional chemotherapy (e.g., doxorubic...   \n",
       "13         Who won the last Liverpool football match?   \n",
       "14  What is the current exchange rate of USD to Eg...   \n",
       "15             What is the recipe for chocolate cake?   \n",
       "\n",
       "                                               answer  \n",
       "0   There is no standard treatment for Erdheim‚ÄìChe...  \n",
       "1   The most common mutation associated with Erdhe...  \n",
       "2   Erdheim-Chester disease and Langerhans cell hi...  \n",
       "3   Interferon-alpha is used to treat some types o...  \n",
       "4   Erdheim‚ÄìChester disease can cause complication...  \n",
       "5   The differential diagnoses for osteoporosis in...  \n",
       "6   BRAF inhibitors are approved for the treatment...  \n",
       "7   If no BRAF mutation is detected, other targete...  \n",
       "8   Radiation therapy is not typically used to tre...  \n",
       "9   Erdheim-Chester disease is a rare disease that...  \n",
       "10  There is no single globally standardized proto...  \n",
       "11                                        Daratumumab  \n",
       "12  Conventional chemotherapy is not a standard tr...  \n",
       "13  As an AI assistant, I can't provide you with r...  \n",
       "14  As of today, the exchange rate of USD to Egypt...  \n",
       "15  Sure, here is a simple recipe for chocolate ca...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(responses)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5832ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"unsloth_model_evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462.3942 seconds used for training.\n",
      "7.71 minutes used for training.\n",
      "Peak reserved memory = 7.893 GB.\n",
      "Peak reserved memory for training = 2.129 GB.\n",
      "Peak reserved memory % of max memory = 53.519 %.\n",
      "Peak reserved memory for training % of max memory = 14.436 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3737adf7",
   "metadata": {},
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d585c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57756964",
   "metadata": {},
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ea19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Continue the fibonnaci sequence.\n",
      "\n",
      "### Input:\n",
      "1, 1, 2, 3, 5, 8\n",
      "\n",
      "### Response:\n",
      "13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, \n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709c126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/vocab.json',\n",
       " 'lora_model/merges.txt',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c2d1e08",
   "metadata": {},
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is a famous tall tower in Paris?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "One of the most famous tall towers in Paris is the Eiffel Tower. It was built by Gustave Eiffel in 1889 and stands at a height of 324 meters (1,063 feet). It is a symbol of Paris and one of the most recognizable structures in the world.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"What is a famous tall tower in Paris?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be1f6e",
   "metadata": {},
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80442000",
   "metadata": {},
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b0d08",
   "metadata": {},
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f60e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaec42c",
   "metadata": {},
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709d4bf",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
    "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
    "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
    "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
    "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
    "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
    "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
    "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
    "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
    "10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
    "11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
    "12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
